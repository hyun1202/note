# 스레드를 직접 사용할 때의 문제점
1. 스레드 생성 시간으로 인한 성능 문제
	* 메모리 할당: 각 스레드는 자신만의 스택을 가지고 있어야 한다.
	* 운영체제 자원 사용: 운영체제 커널 수준에서 이루어지며 시스템 콜을 통해 처리된다. 이는 CPU와 메모리 리소스를 소모하는 작업이다.
	* 운영체제 스케줄러 설정: 스레드가 생성되면 운영체제의 스케줄러는 이 스레드를 관리하고 실행 순서를 조정해야 한다.
	* 스레드 하나는 보통 1MB 이상의 메모리를 사용
2. 스레드 관리 문제
	* CPU, 메모리 자원은 한정되어 있기 때문에 최대 스레드 수 까지만 생성되도록 스레드를 관리해야 한다.
	* **즉 어딘가에 관리가 되어 있어야한다.**
3. `Runnable` 인터페이스의 불편함
	* 스레드의 실행 결과를 받을 수 없다.
	* 체크 예외를 던질 수 없다.

스레드 생성 작업은 상대적으로 무거우므로 **미리 생성하고 생성한 스레드를 재사용하는 방법**을 고려할 수 있다.
	처음 생성할 때를 제외하고는 생성을 위한 시간이 들지 않는다.

## 스레드 풀
* 1번, 2번 문제를 해결하기 위해 **생성하고 관리하는 스레드 풀이 필요**하다.
* 컬렉션에 스레드를 보관하고 재사용할 수 있게 하면 된다.
* **처리할 작업이 없다면 대기(`WAITING`)하고 작업 요청이 오면 실행(`RUNNABLE`) 상태로 변경**한다.
* 하지만 생산자 소비자 문제까지 겹친다.

## Executor 프레임워크
* 생산자 소비자 문제, 스레드 생성, 관리 등을 모두 해결해주는 도구이다.
* 멀티스레딩 및 병렬 처리를 쉽게 사용할 수 있도록 돕는 기능의 모음
* 요청 스레드가 결과를 받아야 하는 상황이면 **`Callable`을 사용하여 받을 수 있다.**

### Callable
* `Runnable`은 체크 예외를 던질 수 없다.
* `java.util.concurrent`에서 제공되는 기능이다.
* `call()`은 반환 타입이 제네릭`V`이다.
* `throws Exception`이므로 체크 예외도 던질 수 있다.
```java
ExecutorService es = Executors.newFixedThreadPool(1);  
// submit()을 이용하여 Callable 작업을 전달할 수 있다.  
Future<Integer> future = es.submit(new MyCallable()); 
Integer result = future.get();  
log("result value = " + result);  
es.close();
```
### Future
* 다른 스레드가 메서드를 실행해서 **즉시 결과를 받는 것은 불가능**하다.
* 이런 이유로 결과를 나중에 받을 수 있는 `Future`라는 객체를 대신 제공한다.
* `Future` 객체 안에 실행 객체의 인스턴스를 보관하고 내부에 작업의 완료 여부와 결과 값을 가진다.
* `Future`의 구현체인 `FutureTask`는 `Runnable` 인터페이스도 함께 구현한다.

#### 작동 방식
1. taskA 작업을 완료한다.
2. `Future`에 반환 값을 담는다.
3. `Future`의 작업 완료 상태를 변경한다.
4. 요청 스레드를 깨운다.

#### Future 없이 바로 반환이 된다면?
```java
// 아래 예제는 없는 문법이다.
Integer result1 = es.submit(task1);
Integer result2 = es.submit(task2);
```
task1 완료 이후 task2가 실행이 된다.

-> Future는 스레드를 먼저 실행하고, 그 이후에 값을 받아올 수 있는 것이다.

#### Future 잘못된 사용법
```java
Future<Integer> f1 = es.submit(task1); // 2초
Integer result1 = f1.get();

Future<Integer> f2 = es.submit(task2); // 2초
Integer result2 = f2.get();
```

`Future`을 사용하지 않은 것과 동일하다.

```
16:31:35.100 [pool-1-thread-1] 작업 시작
16:31:37.123 [pool-1-thread-1] 작업 완료 result=1275  # 2초
16:31:37.125 [pool-1-thread-2] 작업 시작
16:31:39.130 [pool-1-thread-2] 작업 완료 result=3775  # 2초
16:31:39.133 [     main] task1.result= 1275
16:31:39.134 [     main] task2.result= 3775
16:31:39.135 [     main] task1 + task2 = 5050
16:31:39.135 [     main] End
```

즉, 2초씩 소요되어 총 4초의 시간이 걸린 것을 볼 수 있다.

#### 올바른 Future 사용법
* 작업을 모두 `submit()` 하고 그 다음에 `future.get()`을 이용하여 값을 가져온다.

### 정리
* 결과를 받을 때 까지 요청 스레드는 아무 일도 못하고 대기해야 한다.
* `Future` 개념 덕분에 대기하지 않고 다른 작업을 수행할 수 있다.
* 요청 스레드를 블로킹 상태로 만들지 않고 필요한 요청을 모두 수행할 수 있게 해준다.

## ExecutorService
* 여러 작업을 한 번에 편리하게 처리하는 `invokeAll()`, `invokeAny()` 기능 제공

* `void execute(Runnable command)`: `Runnable` 작업 제출, 반환 값이 없다.
* `<T> Future<T> submit(Callable<T> task)`: `Callable` 작업을 제출하고 결과를 반환받는다.
* `Future<?> submit(Runnable task)`: `Runnable` 작업을 제출하고 결과를 반환 받는다.
	* `Runnable은` 반환 결과가 없지만 작업 완료 여부를 확인하기 위해 사용
* `invokeAll(Collection<? extends Callable<T>> tasks)`: 모든 `Callable` 작업을 제출하고, 모든 작업이 완료될 때까지 기다린다.
* `invokeAny(Collection<? extends Callable<T>> tasks)``: 하나의 `Callable`작업이 완료될 때까지 기다리고, 가장 먼저 완료된 작업의 결과를 반환
  완료되지 않은 나머지 작업은 취소한다.

## Graceful shutdown
* 고객의 주문을 처리하고 있는 도중 갑자기 재시작된다면 해당 고객의 주문이 제대로 진행되지 않는다.
* 가장 이상적인 방향은 새로운 주문 요청은 막고, 이미 진행 중인 주문은 모두 완료한 다음 서버를 재시작 하는 것이다.
* 이렇게 문제없이 우아하게 종료하는 방식을 우아한 종료라 한다.

### ExecutorService의 종료 메서드

서비스 종료
* `void shutdown()`: 새로운 작업을 받지 않고, 이미 제출된 작업을 모두 완료한 후에 종료
  논 블로킹 메서드
* `List<Runnable> shutdownNow()`: 실행 중인 작업을 중단하고, 대기 중인 작업을 반환하며 즉시 종료
  논 블로킹 메서드

서비스 상태 확인
* `boolean isShutdown()`: 서비스 종료 여부 확인
* `boolean isTerminated()`: `shutdown()`, `shutdownNow()` 호출 후, 모든 작업이 완료되었는지 확인

작업 완료 대기
* `boolean awaitTermination()`: 서비스 종료 시 지정된 시간까지 모든 작업이 완료될 때까지 대기
  **블로킹** 메서드

close()
* 자바 19부터 지원하는 서비스 종료 메서드로 `shutdown()`과 비슷
* 정확히는 `shutdown()`을 호출하고 하루를 기다려도 작업이 완료되지 않으면 `shutdownNow()`를 호출
* 인터럽트가 발생해도 `shutdownNow()` 호출

### Executor 스레드 풀 관리
`ExecutorService`의 기본 구현체인 `ThreadPoolExecutor`의 생성자
* `corePoolSize`: 스레드 풀에서 관리되는 기본 스레드의 수
* `maximumPoolSize`: 스레드 풀에서 관리되는 최대 스레드 수
* `keepAliveTime`, `TimeUnit unit`: 기본 스레드 수를 초과해 만들어진 **초과 스레드가 생존할 수 있는 대기 시간**, 이 시간 동안 처리할 작업이 없다면 초과 스레드는 제거
* `BlockingQueue workQueue`: 작업을 보관할 블로킹 큐

초과 스레드
* **대기하는 작업까지 꽉 찼을 때**, maximumPoolSize까지 초과 스레드를 만들어서 작업을 수행
  대기 큐까지 꽉 찼을 때는, 이미 스레드 풀에있는 스레드가 전부 실행 중이라는 뜻이다.
* 초과 스레드 개수는 `max - core`
* 이 max 사이즈까지 모두 초과하면 요청을 거절한다. (예외 발생)
  큐도 가득차고, 최대 생성 가능한 스레드 수도 가득찼다.

#### 고정 풀 전략
#####  newSingleThreadPool(): 단일 스레드 풀 전략
* 스레드 풀에 기본 스레드 1개만 사용
* 큐 사이즈에 제한이 없음(LinkedBlockingQueue)
* 주로 간단히 사용하거나, 테스트 용도로 사용
```java
new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>());
```

##### newFixedThreadPool(nThreads): 고정 스레드 풀 전략
* 스레드 풀에 `nThreads` 만큼의 기본 스레드를 생성한다. 
* 초과 스레드는 생성하지 않는다.
* 큐 사이즈에 제한이 없다.
* 스레드 수가 고정되어 있어 CPU, 메모리 리소스가 어느정도 예측 가능한 안정적인 방식이다.
```java
new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<>());
```

단점
* 점진적인 사용자 확대로 서비스 응답이 점점 느려진다.
* 갑작스런 요청 증가로 사용자가 폭증하여 응답을 받지 못한다.
-> 요청이 처리되는 시간보다 쌓이는 시간이 더 빠르다.
서버의 자원은 여유가 있는데, 스레드 수가 고정되어 있어 작업의 처리 속도가 느려 문제가 발생한 것이다.

#### 캐시 풀 전략
##### newCachedThreadPool(): 캐시 스레드 풀 전략
* 기본 스레드를 사용하지 않고, 60초 생존 주기를 가진 **초과 스레드만 사용**
* **초과 스레드의 수는 제한이 없다.**
* 큐에 작업을 저장하지 않고 스레드 풀의 소비자 스레드가 **직접 받아 바로 처리**한다.
* 모든 요청이 대기하지 않고 스레드가 바로바로 처리 (buffer X)
* **매우 빠르고 유연한 전략, 자원만 허용한다면 시스템의 자원을 최대로 사용 가능**
```java
new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue<Runnable>());
```

**캐시 스레드 풀 관리**
1. 작업을 요청하면 core 사이즈 만큼 스레드를 만든다.
	이 때, core 사이즈가 없으므로 바로 core 사이즈를 초과한다.
2. core 사이즈를 초과하면 큐에 작업을 넣는다.
	`SynchronousQueue`는 큐의 저장 공간이 0인 특별한 큐이므로, **큐를 반드시 초과하게 된다.**
3. 큐를 초과하면 max 사이즈 만큼 초과 스레드를 만든다.
	풀에 대기하는 초과 스레드가 있으면 재사용된다.
4. max 사이즈를 초과하면 요청을 거절한다. (예외 발생)
	max 사이즈가 무제한이므로 초과 스레드를 무제한으로 만들 수 있다.

`SynchronousQueue`
* 내부에 저장 공간이 없고 **생산자의 작업을 소비자 스레드에게 직접 전달**한다.
* 소비자 작업을 요청하면 기다리던 생산자가 소비자에게 직접 작업을 전달하고 반환, 그 반대의 경우도 같다.
* 버퍼를 두지 않는 스레드간 직거래

##### 단점
* 점진적인 사용자 확대: 사용자가 점점 증가하면서 스레드 사용량도 함께 늘어나므로 CPU, 메모리의 사용량도 자연스럽게 증가한다.
* 갑작스런 요청 증가: 사용자가 폭증하면 응답을 받지 못할 수 있다.
	* cpu 사용량이 100%이고, 메모리 사용량도 지나치게 높아져있다.
	* 스레드 수가 수 천개 실행되고있고, 너무 많은 스레드가 작업을 처리하면서 시스템 전체가 느려진다.
	* 시스템은 너무 많은 스레드에 잠식 당해 거의 다운
	* 시스템이 멈추는 장애가 발생

캐시 스레드 풀 전략은 서버의 자원을 최대한 사용하지만, **서버가 감당할 수 있는 임계점을 넘는 순간 시스템이 다운될 수 있다.**
#### 사용자 정의 풀 전략
**시나리오**
사용자가 점점 늘어나고, 갑자기 사용자가 폭증한 상황
* 일반: 일반적인 상황에는 자원을 예측할 수 있도록 고정 크기의 스레드로 서비스를 안정적으로 운영
* 긴급: 사용자의 요청이 갑자기 증가하면 긴급하게 스레드를 추가로 투입하여 작업을 빠르게 처리
* 거절: 사용자의 요청이 폭증해서 긴급 대응도 어렵다면 사용자의 요청 거절 (처리 가능한 수준의 사용자 요청만 처리)

세분화 전략
```java
ThreadPoolExecutor es = new ThreadPoolExecutor(100, 200, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue<>(1000));
```
* 100개의 기본 스레드 사용
* 추가로 긴급 대응 가능한 긴급 스레드 100개 사용. 긴급 스레드는 60초의 생존 주기
* 1000개의 작업이 큐에 대기

* 일반: 1000개 이하의 작업 -> 100개의 기본 스레드가 처리 (1100 / 100 = 11초)
* 긴급: 큐에 담긴 작업이 1000개 초과 -> 100개의 기본 스레드 + 100개의 초과 스레드가 처리 (1200 / 200 = 6초)
* 거절: 초과 스레드를 투입했지만, 큐에 담긴 작업을 초과하고 초과 스레드도 넘어간 상황. 이 경우 예외 발생

##### 자주하는 실수
```java
ThreadPoolExecutor es = new ThreadPoolExecutor(100, 200, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue<>());
```
* 기본 스레드 100개
* 최대 스레드 200개
* 큐 사이즈 무한대: 기본 생성자를 지정하면 무한대의 사이즈가 된다. (`new ArrayBlockingQueue<>()`)

큐가 가득 차야 초과 스레드가 생성되는데, 무한대의 사이즈를 사용하게 되면 큐가 가득찰 수가 없다.
결국 기본 스레드 100개만으로 무한대의 작업을 처리해야 하는 문제가 발생

## Executor 예외 정책
* 큐도 가득 차고, 초과 스레드도 더는 할당할 수 없다면 작업을 거절한다.
* 작업을 거절하는 다양한 정책이 있다.

### AbortPolicy
새로운 작업을 제출할 때 `RejectedExecutionException` 발생 (기본)
```java
// 마지막 파라미터에 예외 정책 지정
ThreadPoolExecutor es = new ThreadPoolExecutor(1, 1, 0, TimeUnit.SECONDS, new SynchronousQueue<>(), new ThreadPoolExecutor.AbortPolicy());
```
기본 정책이므로 생략해도 된다.

### DiscardPolicy
새로운 작업을 조용히 버린다.
```java
ThreadPoolExecutor es = new ThreadPoolExecutor(1, 1, 0, TimeUnit.SECONDS, new SynchronousQueue<>(), new ThreadPoolExecutor.DiscardPolicy());
```
해당 정책의 구현부는 비어있다.

### CallerRunsPolicy
새로운 작업을 제출한 스레드가 대신하여 직접 작업 수행
```java
ThreadPoolExecutor es = new ThreadPoolExecutor(1, 1, 0, TimeUnit.SECONDS, new SynchronousQueue<>(), new ThreadPoolExecutor.CallerRunsPolicy());
```

```
22:44:15.101 [     main] task2 시작 # main이 직접 수행
22:44:15.101 [pool-1-thread-1] task1 시작
22:44:16.108 [pool-1-thread-1] task1 완료
22:44:16.108 [     main] task2 완료 
22:44:16.109 [     main] task3 시작 # main이 직접 수행
22:44:17.115 [     main] task3 완료
22:44:17.117 [pool-1-thread-1] task4 시작
22:44:18.121 [pool-1-thread-1] task4 완료
```

생산자 스레드가 대신 일을 수행하는 것도 있지만, 생상자 스레드가 대신 일을 수행하는 덕분에 작업의 생산 자체가 느려진다.
덕분에 작업의 생산 속도가 너무 빠르다면, **생산 속도를 조절할 수 있다.**

### 사용자 정의(`RejectedExecutionHandler`) 
개발자가 직접 정의한 거절 정책 사용
```java
ThreadPoolExecutor es = new ThreadPoolExecutor(1, 1, 0, TimeUnit.SECONDS, new SynchronousQueue<>(), new MyRejectedExecutionHandler());

static class MyRejectedExecutionHandler implements RejectedExecutionHandler {  
    static AtomicInteger count = new AtomicInteger(0);  
    @Override  
    public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {  
        int i = count.incrementAndGet();  
        log("누적된 거절 작업 수: " + i);  
    }  
}
```

`RejectedExecutionHandler`를 구현한다.


-------
## 비동기 관련 인사이트
여태까지 `Future`나 `Promise`를 이용하여 `await(node)`이나 `future.get(java)`을 사용했을 때, 비동기라고는 하지만 결국엔 동기적인 방식으로 작동했기에 이것이 비동기가 맞는가? 라는 것에 대한 의문이 있었다.

하지만 결론적으로 헷갈렸던 것은 비동기 방식과 논블로킹이었다.
비동기적으로 실행시키기 위해 다른 스레드를 이용한 것이지만, 결국 값을 받기 위해 메인 스레드는 `BLOCKED`가 되어야 했던 것이다. 결국 **`get()`을 이용한 방식은 블로킹 방식**이 되는 것이다.
하지만 해당 메서드가 실행될 때는 **비동기 방식**으로 처리가 된다.
즉, 비동기 방식이나 블로킹 방식인 것이다.
`future.get()`를 사용하지 않는 것은, 비동기 방식이나 논 블로킹 방식인 것이다.

하지만, await()는 node기반의 싱글 스레드로 동작하여 메인 스레드는 차단되지 않는다.

즉, 비동기, 동기는 실행 방식의 문제이고 
블로킹, 논블로킹의 차이는 스레드 상태의 문제이다.